import requests
from bs4 import BeautifulSoup
import urllib.parse

def resolve_baidu_url(baidu_link):
    """å°†ç™¾åº¦è·³è½¬é“¾æ¥è¿˜åŸä¸ºçœŸå®é“¾æ¥"""
    try:
        resp = requests.get(baidu_link, allow_redirects=True, timeout=5)
        return resp.url
    except:
        return baidu_link

def extract_web_content(url, max_length=1500):
    """æå–ç½‘é¡µæ­£æ–‡å†…å®¹ï¼Œè¿‡æ»¤ä¹±ç """
    import re
    
    def clean_text(text):
        """æ¸…ç†æ–‡æœ¬ï¼Œåªä¿ç•™ä¸­æ–‡ã€è‹±æ–‡å­—ç¬¦"""
        cleaned_chars = []
        
        for char in text:
            # åªä¿ç•™ä¸­æ–‡ã€è‹±æ–‡å­—æ¯ã€æ•°å­—ã€åŸºæœ¬æ ‡ç‚¹å’Œç©ºæ ¼
            if ('\u4e00' <= char <= '\u9fff' or          # ä¸­æ–‡å­—ç¬¦
                'a' <= char <= 'z' or                    # è‹±æ–‡å°å†™
                'A' <= char <= 'Z' or                    # è‹±æ–‡å¤§å†™
                '0' <= char <= '9' or                    # æ•°å­—
                char in ' \n\t.,!?;:()[]{}"-\''):        # åŸºæœ¬æ ‡ç‚¹å’Œç©ºæ ¼
                cleaned_chars.append(char)
        
        # åˆå¹¶è¿ç»­çš„ç©ºç™½å­—ç¬¦
        cleaned_text = ''.join(cleaned_chars)
        lines = cleaned_text.split('\n')
        
        # è¿‡æ»¤ç©ºè¡Œå’Œè¿‡çŸ­çš„è¡Œ
        meaningful_lines = []
        for line in lines:
            line = line.strip()
            if len(line) > 2:  # è‡³å°‘3ä¸ªå­—ç¬¦æ‰ä¿ç•™
                meaningful_lines.append(line)
        
        return '\n'.join(meaningful_lines)
    
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        resp = requests.get(url, headers=headers, timeout=8)
        
        # å°è¯•æ­£ç¡®è§£ç 
        resp.encoding = resp.apparent_encoding or 'utf-8'
        
        soup = BeautifulSoup(resp.text, "html.parser")
        
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()
        
        text = soup.get_text(separator='\n', strip=True)
        
        # æ¸…ç†ä¹±ç 
        cleaned_text = clean_text(text)
        
        # ç¡®ä¿è¿”å›æŒ‡å®šé•¿åº¦çš„æœ‰æ•ˆå†…å®¹
        return cleaned_text[:max_length] if cleaned_text else None
        
    except Exception as e:
        print(f"æŠ“å–å¤±è´¥: {url} - {e}")
        return None

def baidu_search(query, max_results=5):
    # å°†ä¸­æ–‡è½¬ä¸ºURLç¼–ç 
    query_encoded = urllib.parse.quote(query)
    url = f"https://www.baidu.com/s?wd={query_encoded}"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) "
                      "Chrome/114.0.0.0 Safari/537.36"
    }

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print(f"è¯·æ±‚å¤±è´¥: {response.status_code}")
        return []

    soup = BeautifulSoup(response.text, "html.parser")

    results = []
    for div in soup.find_all("div", class_="result")[:max_results]:
        a_tag = div.find("a")
        if a_tag and a_tag.get("href"):
            title = a_tag.get_text(strip=True)
            href = a_tag.get("href")
            results.append({"title": title, "url": href})

    return results

# âœ… è°ƒç”¨æœç´¢ + æå–æ­£æ–‡
if __name__ == "__main__":

    query = "å…°å·å¤§å­¦2025å¹´æ‹›ç”Ÿç®€ç« "
    results = baidu_search(query, max_results=3)

    for item in results:
        print(f"ğŸ“Œ æ ‡é¢˜: {item['title']}")
        print(f"ğŸ”— è·³è½¬é“¾æ¥: {item['url']}")

        real_url = resolve_baidu_url(item["url"])
        print(f"ğŸŒ çœŸå®é“¾æ¥: {real_url}")

        content = extract_web_content(real_url)
        if content:
            print("ğŸ“„ æ­£æ–‡å†…å®¹ï¼ˆå‰300å­—ï¼‰:")
            print(content[:300])
        print("-" * 60)
